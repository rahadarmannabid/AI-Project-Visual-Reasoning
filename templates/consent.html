<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Consent Form</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }
        .consent-form {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            width: 60%;
            max-width: 600px;
            margin: auto;
        }
        .consent-form h2 {
            color: #333;
        }
        .consent-form p {
            color: #666;
        }
        .consent-button {
            background-color: #4CAF50;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        .consent-button:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <div class="consent-form">
        <h2>Research Project Consent Form</h2>
        <p>
            <strong>Title:</strong> Contextual Visual Reasoning<br>
            <em>Contextual Visual Reasoning</em> is a research project conducted by Rahad Arman Nabid for a course project.
        </p>

        <h3>Details about the Research Project:</h3>
        <p>
            People with visual impairments or limited vision often encounter difficulties in interpreting images due to difficulties distinguishing between different objects, especially from the Web. This difficulty is heightened when confronted with intricate images and the need for advanced reasoning to understand them. Despite these individuals making up approximately 8% of the population, there remains a significant gap in tools that offer contextual explanations from visual data. Our project aims to bridge this gap by developing a vision-language model designed to generate context-driven interpretations of visual tasks based on specific questions.
        </p>

        <h3>Prototype:</h3>
        <p>
            In this user survey, we will provide you with a set of images with reasoning questions. The images are modified to simulate the experience of a person with visual impairment. You will be asked to answer questions based on your understanding of the image. The questions are designed to be answerable without any prior knowledge of the image. If you click on the questions, our system will show you the answers to help you understand the image better. You can also ask questions to the system, and it will provide answers based on the image. Your focus will be on evaluating how well the system presents different reasoning questions and the quality of the answers.
        </p>

        <h3>Time:</h3>
        <p>
            The survey will take approximately 20 minutes to complete.
        </p>

        <h3>Post Survey:</h3>
        <p>
            After completing the survey, you will be asked to fill out a post-survey questionnaire. The post-survey questionnaire will assess your understanding of the image and the quality of the answers. It will take approximately 5 minutes to complete.
        </p>

        <h3>Privacy:</h3>
        <p>
            Your answers will be treated with utmost confidentiality, and we will not gather any personally identifiable information. Your responses will solely be utilized for research objectives. All data will be securely stored, with access limited solely to the researcher. Following the conclusion of the research project, the data will be securely deleted. An email address will be requested solely for response identification purposes.
        </p>

        <form action="/next_page" method="post">
            <input type="submit" value="I Consent" class="consent-button">
        </form>
    </div>
</body>
</html>
